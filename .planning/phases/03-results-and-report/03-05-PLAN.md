---
phase: 03-results-and-report
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - src/app/api/checkout/route.ts
  - src/lib/benchmark/runner.ts
autonomous: false
gap_closure: true

must_haves:
  truths:
    - "Mock checkout triggers benchmark execution so benchmark_runs rows are created"
    - "Runner mock mode activates when DEBUG_MOCK_OPENROUTER=true is set in .env.local"
    - "Live progress page shows real-time model completion when using mock mode"
  artifacts:
    - path: "src/app/api/checkout/route.ts"
      provides: "Mock checkout path calls runBenchmark via after()"
      contains: "after"
    - path: "src/lib/benchmark/runner.ts"
      provides: "Mock mode check uses canonical DEBUG_MOCK_OPENROUTER env var"
      contains: "isMockOpenRouter"
  key_links:
    - from: "src/app/api/checkout/route.ts"
      to: "src/lib/benchmark/engine.ts"
      via: "after(() => runBenchmark(report.id))"
      pattern: "runBenchmark\\(report\\.id\\)"
    - from: "src/lib/benchmark/runner.ts"
      to: "src/lib/debug/mock-config.ts"
      via: "import { isMockOpenRouter }"
      pattern: "import.*isMockOpenRouter.*from.*mock-config"
---

<objective>
Fix three compounding bugs that prevent the entire mock-mode development flow from working: (1) mock checkout never triggers benchmark execution, (2) runner checks wrong env var for mock mode, (3) document migration 004 requirement.

Purpose: UAT revealed that mock-mode benchmarks never run -- the report sits at "paid" forever with zero benchmark_runs rows. This blocks all Phase 3 human verification (live progress, report page, charts, everything).

Output: Two files patched so mock checkout triggers runBenchmark() and the runner correctly detects mock OpenRouter mode via DEBUG_MOCK_OPENROUTER.
</objective>

<execution_context>
@/Users/lukelibraro/.claude/get-shit-done/workflows/execute-plan.md
@/Users/lukelibraro/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/app/api/checkout/route.ts
@src/lib/benchmark/runner.ts
@src/lib/debug/mock-config.ts
@src/app/api/webhooks/stripe/route.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add after(runBenchmark) to mock checkout path and fix runner env var</name>
  <files>src/app/api/checkout/route.ts, src/lib/benchmark/runner.ts</files>
  <action>
Two files need surgical fixes:

**File 1: src/app/api/checkout/route.ts**

Add `after` import from `next/server` (add it to the existing NextRequest/NextResponse import on line 1):
```ts
import { NextRequest, NextResponse, after } from "next/server";
```

After the mock report is created and draft status is updated (after line 167, before the return on line 169), add the exact same `after()` call that the webhook uses at lines 163-170 of `src/app/api/webhooks/stripe/route.ts`:
```ts
// Trigger benchmark execution (mirrors webhook behavior)
after(async () => {
  try {
    const { runBenchmark } = await import("@/lib/benchmark/engine");
    await runBenchmark(report.id);
  } catch (err) {
    console.error("[checkout:mock:after] Benchmark engine error:", err);
  }
});
```

This uses dynamic import for the engine module (same pattern as the webhook) so the checkout route doesn't statically depend on the heavy engine module.

**File 2: src/lib/benchmark/runner.ts**

Replace the `isMockOpenRouter()` function (lines 53-58) to import from the canonical mock-config module instead of checking its own wrong env var:

Remove the existing function:
```ts
export function isMockOpenRouter(): boolean {
  return (
    process.env.NEXT_PUBLIC_MOCK_OPENROUTER === "true" ||
    process.env.NEXT_PUBLIC_MOCK_OPENROUTER === "1"
  );
}
```

Replace with a re-export from mock-config:
```ts
import { isMockOpenRouter } from "@/lib/debug/mock-config";
export { isMockOpenRouter };
```

Move the new import to the top of the file with the other imports (after line 3). The re-export stays where the old function was (to maintain the same export surface for other consumers). Actually, since isMockOpenRouter is exported and used locally within the same file (line 226), the simplest approach is:

1. Add `import { isMockOpenRouter } from "@/lib/debug/mock-config";` to the imports section (after line 3)
2. Remove the entire `isMockOpenRouter` function definition (lines 52-58) and its JSDoc comment (lines 49-58)
3. Keep `export { isMockOpenRouter };` in place of the removed function to preserve the public API

This ensures runner.ts uses `DEBUG_MOCK_OPENROUTER` (via mock-config.ts) instead of the nonexistent `NEXT_PUBLIC_MOCK_OPENROUTER` env var.
  </action>
  <verify>
1. Run `npx tsc --noEmit` -- must pass with zero errors (confirms import paths and types are correct)
2. Grep for NEXT_PUBLIC_MOCK_OPENROUTER in runner.ts -- must return zero results
3. Grep for "after(" in checkout/route.ts -- must find the new after() call
4. Grep for "runBenchmark" in checkout/route.ts -- must find the dynamic import
  </verify>
  <done>
- Mock checkout POST creates report AND triggers runBenchmark via after()
- Runner detects mock mode via DEBUG_MOCK_OPENROUTER (the env var actually set in .env.local)
- Both changes are minimal and follow existing patterns (webhook's after() pattern, mock-config's canonical env var)
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Verify mock-mode benchmark pipeline works end-to-end</name>
  <files></files>
  <action>
Human verifies the full mock-mode flow works after the code fixes.
  </action>
  <verify>
**Pre-requisite: Verify migration 004 is applied**

Run this SQL in your Supabase SQL Editor to check if the Realtime publication includes the needed tables:
```sql
SELECT tablename FROM pg_publication_tables WHERE pubname = 'supabase_realtime';
```
If `benchmark_runs` and `reports` are NOT listed, run `supabase/migrations/004_realtime_and_shared_runs.sql` in the SQL Editor.

**Test the fix:**

1. Start the dev server: `npm run dev`
2. Ensure `.env.local` has `DEBUG_MOCK_STRIPE=true` and `DEBUG_MOCK_OPENROUTER=true`
3. Go through the wizard flow and click "Run Benchmark" (mock checkout)
4. You should land on the processing page (`/benchmark/[id]/processing`)
5. Verify:
   - Connection status shows "Connected" (green), NOT "Reconnecting" (yellow)
   - Per-model progress bars appear and fill as mock benchmark runs complete (500-2000ms each)
   - Page auto-redirects to `/report/[token]` when all runs finish
6. On the report page, verify charts and data render with mock benchmark results

If connection shows "Reconnecting", migration 004 likely needs to be applied (see pre-requisite above).
  </verify>
  <done>Live progress displays real-time model completion in mock mode. Report page renders with mock benchmark data. The Phase 3 UAT blocker is resolved.</done>
</task>

</tasks>

<verification>
- `npx tsc --noEmit` passes (type safety)
- `grep -r "NEXT_PUBLIC_MOCK_OPENROUTER" src/lib/benchmark/runner.ts` returns nothing
- `grep "after(" src/app/api/checkout/route.ts` finds the new after() call
- `grep "runBenchmark" src/app/api/checkout/route.ts` finds the dynamic import
- No other files modified (minimal surgical fix)
</verification>

<success_criteria>
Mock checkout triggers benchmark execution. Runner correctly detects DEBUG_MOCK_OPENROUTER. Live progress page shows real-time model completion. Report page renders with mock data. The entire Phase 3 test suite is unblocked.
</success_criteria>

<output>
After completion, create `.planning/phases/03-results-and-report/03-05-SUMMARY.md`
</output>
